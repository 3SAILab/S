import random
import os
import torch
import matplotlib.pyplot as plt


def set_seed(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed) # if using multi-GPU
        # Ensure deterministic behavior (may impact performance)
        # torch.backends.cudnn.deterministic = True
        # torch.backends.cudnn.benchmark = False

def plot_loss(losses, steps, output_file="loss_curve.png", title="Training Loss Curve", beta=0.1):
    if not steps:
        print("No loss data recorded.")
        return
    # Calculate exponential moving average
    ema_losses = []
    if losses:
        ema_losses = [losses[0]]  # Initial value
        for i in range(1, len(losses)):
            ema = beta * losses[i] + (1 - beta) * ema_losses[-1]
            ema_losses.append(ema)
    
    plt.figure(figsize=(10, 6), dpi=150)
    plt.title(title)
    plt.fill_between(steps, losses, ema_losses, alpha=0.2, color="#1f77b4")
    plt.plot(steps, ema_losses, label='Smoothed Loss (EMA)', linewidth=1, color="#1f77b4")
    plt.title(title)
    plt.xlabel("Training Steps")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True, alpha=0.7)
    plt.tight_layout()
    plt.savefig(output_file)
    print(f"Loss curve saved to {output_file}")

def plot_lr(learning_rates, steps, output_file="lr_curve.png", title="Learning Rate Schedule"):
    """
    ç»˜åˆ¶å­¦ä¹ ç‡å˜åŒ–æ›²çº¿
    
    å‚æ•°:
        learning_rates: å­¦ä¹ ç‡åˆ—è¡¨
        steps: è®­ç»ƒæ­¥éª¤åˆ—è¡¨
        output_file: è¾“å‡ºå›¾åƒæ–‡ä»¶è·¯å¾„
        title: å›¾è¡¨æ ‡é¢˜
    """
    if not steps or not learning_rates:
        print("æ²¡æœ‰å­¦ä¹ ç‡æ•°æ®å¯ç»˜åˆ¶ã€‚")
        return
        
    plt.figure(figsize=(10, 6), dpi=150)
    plt.plot(steps, learning_rates, linewidth=2, color="#2ca02c")
    plt.title(title)
    plt.xlabel("Training Steps")
    plt.ylabel("Learning Rate")
    plt.grid(True, alpha=0.7)
    # plt.yscale('log')  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦æ›´å¥½åœ°æ˜¾ç¤ºå­¦ä¹ ç‡å˜åŒ–
    plt.tight_layout()
    plt.savefig(output_file)
    print(f"Learning Rate curve saved to {output_file}")


def print_model_parameters(model, min_param_count=10000, max_depth=2):
    """
    ç¾è§‚ç²¾ç®€åœ°æ‰“å°æ¨¡å‹å‚æ•°ä¿¡æ¯
    
    å‚æ•°:
        model: è¦åˆ†æçš„æ¨¡å‹
        min_param_count: åªæ˜¾ç¤ºå‚æ•°æ•°é‡å¤§äºæ­¤å€¼çš„æ¨¡å— (é»˜è®¤10,000)
        max_depth: æœ€å¤§æ˜¾ç¤ºæ·±åº¦ (é»˜è®¤2)
    """
    print("\nğŸ“Š Model Parameters Summary")
    print("=" * 60)
    
    total_params = 0
    module_info = {}
    layer_params = 0
    layer_submodules = set()
    layer_count = 0  # æ–°å¢ï¼šè®°å½•layerå±‚æ•°
    
    # æ”¶é›†æ¨¡å—ä¿¡æ¯
    for name, param in model.named_parameters():
        param_count = param.numel()
        total_params += param_count
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯layeræ¨¡å—
        parts = name.split('.')
        if len(parts) > 1 and parts[0] == 'layers':
            # åˆå¹¶æ‰€æœ‰layersçš„å‚æ•°
            layer_params += param_count
            if len(parts) > max_depth:
                layer_submodules.add('.'.join(parts[1:2] + parts[max_depth:max_depth+1]))  # åŒ…å«layerç¼–å·
            layer_count = max(layer_count, int(parts[1])) + 1  # ç»Ÿè®¡layeræ•°é‡
            continue
        
        # æå–æ¨¡å—è·¯å¾„
        module_path = '.'.join(parts[:max_depth])
        
        # æ›´æ–°æ¨¡å—ä¿¡æ¯
        if module_path not in module_info:
            module_info[module_path] = {
                'count': 0,
                'submodules': set(),
                'params': []
            }
        
        module_info[module_path]['count'] += param_count
        if len(parts) > max_depth:
            module_info[module_path]['submodules'].add('.'.join(parts[max_depth:max_depth+1]))
        module_info[module_path]['params'].append((name, param.shape, param_count))
    
    # å¦‚æœæœ‰layerå‚æ•°ï¼Œæ·»åŠ åˆ°æ¨¡å—ä¿¡æ¯
    if layer_params > 0:
        # è®¡ç®—å®é™…å­æ¨¡å—æ€»æ•°ï¼ˆå±‚æ•°Ã—æ¯å±‚å­æ¨¡å—æ•°ï¼‰
        unique_submodules_per_layer = len(set(s.split('.')[1] for s in layer_submodules)) if layer_submodules else 0
        total_submodules = layer_count * unique_submodules_per_layer if unique_submodules_per_layer > 0 else len(layer_submodules)
        
        # ç¡®ä¿tok_embeddings.weightæ’åœ¨å‰é¢
        if 'tok_embeddings.weight' in module_info:
            module_info = {'tok_embeddings.weight': module_info['tok_embeddings.weight'], 
                          'layers': {
                              'count': layer_params,
                              'submodules': layer_submodules,
                              'params': [],
                              'total_submodules': total_submodules
                          }}
            # æ·»åŠ å…¶ä»–æ¨¡å—
            for k, v in module_info.items():
                if k not in ['tok_embeddings.weight', 'layers']:
                    module_info[k] = v
        else:
            module_info['layers'] = {
                'count': layer_params,
                'submodules': layer_submodules,
                'params': [],
                'total_submodules': total_submodules
            }
    
    # æ‰“å°ä¸»è¦æ¨¡å—
    print(f"{'Module':<30} | {'Params':>15} | {'%':>6}")
    print("-" * 60)
    
    # æŒ‰ç‰¹å®šé¡ºåºæ’åºï¼ˆç¡®ä¿tok_embeddings.weightç¬¬ä¸€ï¼‰
    custom_order = ['tok_embeddings.weight', 'layers']
    remaining_modules = [m for m in module_info.keys() if m not in custom_order]
    sorted_modules = [(k, module_info[k]) for k in custom_order if k in module_info] + \
                    sorted([(m, module_info[m]) for m in remaining_modules], 
                          key=lambda x: -x[1]['count'])
    
    for module, info in sorted_modules:
        if info['count'] < min_param_count:
            continue
            
        param_percent = info['count'] / total_params * 100
        submodule_count = info.get('total_submodules', len(info['submodules']))
        
        # ä¸»æ¨¡å—è¡Œ
        print(f"{module:<30} | {info['count']:>15,} | {param_percent:>5.1f}%")
        
        # å¦‚æœæœ‰å­æ¨¡å—ä¸”å‚æ•°è¾ƒå¤šï¼Œæ˜¾ç¤ºå­æ¨¡å—æ€»æ•°
        if submodule_count > 0 and info['count'] > min_param_count * 10:
            print(f"  â”” [merged {submodule_count} submodules]")
    
    # æ‰“å°æ±‡æ€»ä¿¡æ¯
    print("=" * 60)
    print(f"ğŸ”¹ Total Parameters: {total_params:,} ({total_params/1e6:.2f}M)")
    
    # è®¡ç®—å¹¶æ‰“å°æœªæ˜¾ç¤ºçš„å°å‚æ•°
    shown_params = sum(info['count'] for _, info in sorted_modules if info['count'] >= min_param_count)
    if shown_params < total_params:
        small_params = total_params - shown_params
        print(f"ğŸ”¸ Small params (<{min_param_count:,}): {small_params:,} ({small_params/total_params:.1%})")
    
    print("=" * 60)
    
    return total_params / 1e6  # è¿”å›æ¨¡å‹å¤§å°ï¼ˆç™¾ä¸‡å‚æ•°ï¼‰
